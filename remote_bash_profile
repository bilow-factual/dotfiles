source /home/bilow/.rvm/scripts/rvm
[[ -s "$HOME/.rvm/scripts/rvm" ]] && . "$HOME/.rvm/scripts/rvm"
# Load RVM function

PATH="/usr/local/bin:/usr/local/sbin:$HOME/bin:/usr/local/spark-1.6.1-bin-hadoop2.6/bin:~/code/nfu:$PATH"
export PATH=/home/bilow/anaconda2/bin:$PATH
PROFILE_DIRECTORY="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"

source $PROFILE_DIRECTORY/.git-prompt.sh
export PS1='\[$(tput bold)\]\[\033[38;5;11m\]\u\[$(tput sgr0)\]@\[$(tput bold)\]\h \[\033[38;5;214m\]\W \[$(tput sgr0)\]\[$(tput bold)\]\[\033[38;5;153m\]$(__git_ps1 "(%s)")\[$(tput sgr0)\]\[$(tput sgr0)\]\$ '

export CLICOLOR=1
export LSCOLORS=EhFxBxDxBxegedabagacad

export PYSPARK_PYTHON='/usr/local/bin/python3'
export HISTSIZE=10000
export HISTFILESIZE=10000

alias grep='grep --color=auto'
alias emacs='emacs -nw'
alias tmux_h='tmux select-layout even-horizontal'
alias tmux_v='tmux select-layout even-vertical'
alias tma='tmux attach-session -t '
alias tml='tmux list-sessions'
alias tmk='tmux kill-session -t '


alias gs='git status '
alias ga='git add '
alias gb='git branch '
alias gc='git commit'
alias gcm='git commit -m '
alias gd='git diff'
alias go='git checkout '
alias gk='gitk --all&'
alias gx='gitx --all'

alias got='git '
alias get='git '
alias gst='git status'
alias haddop='hadoop'
alias adoop='hadoop'
alias yak='yarn application -kill'
alias rmr='rm -r'
alias hdmp='hadoop fs -mkdir -p'
alias hdmv='hadoop fs -mv'
alias hdrm='hadoop fs -rm -r'
alias hddu='hadoop fs -du -h'
alias hdls='hadoop fs -ls -l'
alias hdpt='hadoop fs -put -f'

nbserve () {
  jupyter notebook --no-browser --port=8888
}

[[ -s "$HOME/.rvm/scripts/rvm" ]] && source "$HOME/.rvm/scripts/rvm" # Load RVM into a shell session *as a function*

alias tabfirstsort='sort -t "	" -k 1 -n'
alias update='sudo apt-get update && sudo apt-get upgrade && sudo apt-get dist-upgrade'
alias whatismyip='curl -s checkip.dyndns.org | grep -Eo [0-9.]+'
alias ls="ls --color=always"
export MAVEN_OPTS="-Xmx512m"

export HADOOP_CONF_DIR=/etc/hadoop/conf
export NFU_HADOOP_STREAMING=/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.3.jar
export NFU_HADOOP_OPTIONS="-D mapred.job.map.memory.mb=8192"
export NI_HADOOP_JOBCONF="mapreduce.job.reduces=1024 mapreduce.job.reduce.slowstart.completedmaps=1.0"
export NI_HDFS_TMPDIR=/user/bilow/tmp
export NI_MONITOR=no
export PP="part-*"
export P0="part-0000*"
export UB=/user/bilow
export UBT=/user/bilow/tmp
export UBI=/user/bilow/insights
export UBID=/user/bilow/insights/data
export UBIP=/user/bilow/insights/places
export UBIS=/user/bilow/insights/segments
export UBD=/user/bilow/dqm
export UBDR=/user/bilow/dqm/raw_data
export UBG=/user/bilow/geocoding
export HSC1="^{hadoop/jobconf=mapreduce.job.reduces=1}"
export HSC16=^{hadoop/jobconf='mapreduce.job.reduces=16384 mapreduce.job.reduce.slowstart.completedmaps=1.0'}
export AA=/apps/audience
export AAP=/apps/audience/production
export AAPS=/apps/audience/production/record-split
export AAPSD=/apps/audience/production/record-split/dappack
export AAPSM=/apps/audience/production/record-split/mopubpack
export AAPST=/apps/audience/production/record-split/twcpack
export AE=/apps/extract
export AED=/apps/extract/dqm
export TNP="/tmp/ni-pe-bilow."

cat ~/.pw | kinit
hadoop fs -rm -r .Trash
